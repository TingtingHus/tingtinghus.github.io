<!DOCTYPE html><html lang="zh_hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Python线性回归学习笔记"><meta name="keywords" content="Python,线性回归"><meta name="author" content="Tingting Hu"><meta name="copyright" content="Tingting Hu"><title>Python线性回归学习笔记 | 三味书屋</title><link rel="shortcut icon" href="/images/avatar.jpg"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.3.0'
} </script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="三味书屋" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E5%85%83%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">线性模型(多元回归)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%BA%93statsmodels"><span class="toc-number">1.1.</span> <span class="toc-text">统计学库statsmodels</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8B%A5%E5%90%AB%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F-%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F"><span class="toc-number">1.1.1.</span> <span class="toc-text">若含分类变量(虚拟变量)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A7%91%E5%AD%A6%E5%BA%93-sklearn"><span class="toc-number">1.2.</span> <span class="toc-text">科学库 sklearn</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8B%A5%E5%90%AB%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F-%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F-1"><span class="toc-number">1.2.1.</span> <span class="toc-text">若含分类变量(虚拟变量)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3sklearn%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%8D%E5%B8%A6%E6%A0%87%E7%AD%BE%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.2.</span> <span class="toc-text">解决sklearn回归模型不带标签的问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%EF%BC%88GLM%EF%BC%89%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">逻辑回归（广义线性模型（GLM））</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%BA%93statsmodels-1"><span class="toc-number">2.1.</span> <span class="toc-text">统计学库statsmodels</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8sklearn"><span class="toc-number">2.2.</span> <span class="toc-text">使用sklearn</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%9B%9E%E5%BD%92%EF%BC%88%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%EF%BC%88GLM%EF%BC%89%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">泊松回归（广义线性模型（GLM））</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%BA%93statsmodels-2"><span class="toc-number">3.1.</span> <span class="toc-text">统计学库statsmodels</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%8A%E6%96%AD"><span class="toc-number">4.</span> <span class="toc-text">模型诊断</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%A8%A1%E5%9E%8B%E6%9C%AC%E8%BA%AB%E5%A5%BD%E5%9D%8F%EF%BC%9A%E6%AE%8B%E5%B7%AE"><span class="toc-number">4.1.</span> <span class="toc-text">比较模型本身好坏：残差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%A8%A1%E5%9E%8B%E6%9C%AC%E8%BA%AB%E5%A5%BD%E5%9D%8F%EF%BC%9AQ-Q%E5%9B%BE"><span class="toc-number">4.2.</span> <span class="toc-text">比较模型本身好坏：Q-Q图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E6%95%B0%E6%98%AF%E5%90%A6%E6%98%BE%E8%91%97"><span class="toc-number">4.3.</span> <span class="toc-text">系数是否显著</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%97%B4%E5%A5%BD%E5%9D%8F"><span class="toc-number">4.4.</span> <span class="toc-text">比较模型之间好坏</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.1.</span> <span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.2.</span> <span class="toc-text">广义线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%98%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8K%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E8%AF%84%E5%88%A4%E6%A8%A1%E5%9E%8B%E5%A5%BD%E5%9D%8F"><span class="toc-number">4.4.3.</span> <span class="toc-text">还可以使用K折交叉验证评判模型好坏</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/avatar.ico"></div><div class="author-info__name text-center">Tingting Hu</div><div class="author-info__description text-center">老骥伏枥，志在千里</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">16</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/images/top.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">三味书屋</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Python线性回归学习笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-11</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.3k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="线性模型-多元回归"><a href="#线性模型-多元回归" class="headerlink" title="线性模型(多元回归)"></a>线性模型(多元回归)</h2><h3 id="统计学库statsmodels"><a href="#统计学库statsmodels" class="headerlink" title="统计学库statsmodels"></a>统计学库statsmodels</h3><p>（亦可参考下面系数显著性）</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line">model = smf.ols(formula=<span class="string">'y ~ x1 + x2 +x3'</span>,data=df).fit() <span class="comment">#model即为OLS回归模型,x1为column</span></span><br><span class="line">model.sumamry() <span class="comment"># 结果汇总表</span></span><br><span class="line">model.params <span class="comment"># 获得回归系数</span></span><br><span class="line">model.conf_int() <span class="comment"># 获得置信区间</span></span><br></pre></td></tr></tbody></table></figure>
<p>或</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">X = diabetes.data</span><br><span class="line">y = diabetes.target</span><br><span class="line"></span><br><span class="line">X2 = sm.add_constant(X)</span><br><span class="line">est = sm.OLS(y, X2)</span><br><span class="line">est2 = est.fit()</span><br><span class="line">print(est2.summary())</span><br></pre></td></tr></tbody></table></figure>
<h4 id="若含分类变量-虚拟变量"><a href="#若含分类变量-虚拟变量" class="headerlink" title="若含分类变量(虚拟变量)"></a>若含分类变量(虚拟变量)</h4><p>statsmodels 会自动创建虚拟变量，且删除多余参考变量来避免多重共线性，可以理解为statsmodels 在一定程度上可以减轻多重共线性<br>比如 <code>model = smf.ols(formula='y ~ x1 + x2 +x3 + day',data=df).fit() #model即为OLS回归模型</code> ,回归结果中只有<br>day[T.Fri] \ day[Sat] \ day[Sun] 比原本的 Fri\Sat\Sun\Thur 少了Thur，就说明它就是那个用于解释系数的参考变量</p>
<h3 id="科学库-sklearn"><a href="#科学库-sklearn" class="headerlink" title="科学库 sklearn"></a>科学库 sklearn</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">lr = linear_model.LinearRegression()</span><br><span class="line">predicted = lr.fit(X=df[<span class="string">'x1'</span>,<span class="string">'x2'</span>,<span class="string">'x3'</span>],y=df[<span class="string">'y'</span>])  <span class="comment"># predicted即为线性回归模型</span></span><br><span class="line"><span class="comment"># 注意如果自变量只有一元，则要重塑变量格式，predicted = lr.fit(X=df['x1'].values.reshape(-1,1),y=df['y'].values)</span></span><br><span class="line">predicted.coef_ <span class="comment"># 获得回归自变量系数</span></span><br><span class="line">predicted.intercept_  <span class="comment"># 获得回归截距项（常数项）</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="若含分类变量-虚拟变量-1"><a href="#若含分类变量-虚拟变量-1" class="headerlink" title="若含分类变量(虚拟变量)"></a>若含分类变量(虚拟变量)</h4><p>需要使用get_dummies 函数创建虚拟变量，如</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df_dummy = pd.get_dummies(df[[<span class="string">'x1'</span>,<span class="string">'sex'</span>,<span class="string">'day'</span>]])</span><br><span class="line">print(df_dummy) <span class="comment"># 会生成sex_Male \ sex_Female \day_Fri \ day_Sat \ day_Sun \ day_Thur</span></span><br><span class="line">df_dummy_ref = pd.get_dummies(df[[<span class="string">'x1'</span>,<span class="string">'sex'</span>,<span class="string">'day'</span>]],drop_first=<span class="literal">True</span>) <span class="comment"># drop_first删除虚拟变量</span></span><br><span class="line">print(df_dummy_ref)  <span class="comment"># 会自动去掉参考变量（多重共线性）生成sex_Female \ day_Fri \ day_Sat \ day_Sun</span></span><br><span class="line">lr = linear_model.LinearRegression()</span><br><span class="line">predicted = lr.fit(X=df_dummy_ref,y=df[<span class="string">'y'</span>])</span><br></pre></td></tr></tbody></table></figure>
<h4 id="解决sklearn回归模型不带标签的问题"><a href="#解决sklearn回归模型不带标签的问题" class="headerlink" title="解决sklearn回归模型不带标签的问题"></a>解决sklearn回归模型不带标签的问题</h4><p>sklearn模型结果不带标签（即不会给出系数是哪个自变量的系数），原因是NumPy ndarray无法存储这类元数据，因此需要手动存储标签</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">df_dummy_ref = pd.get_dummies(df[[<span class="string">'x1'</span>,<span class="string">'sex'</span>,<span class="string">'day'</span>]],drop_first=<span class="literal">True</span>)</span><br><span class="line">lr = linear_model.LinearRegression()</span><br><span class="line">predicted = lr.fit(X=df_dummy_ref,y=df[<span class="string">'y'</span>])</span><br><span class="line">values = np.append(predicted.intercept_,predicted.coef_) <span class="comment"># 获取回归系数和截距项（常数项）</span></span><br><span class="line">names = np.append(<span class="string">'intercept'</span>,df_dummy_ref.columns)  <span class="comment"># 获取值对应名称</span></span><br><span class="line">results = pd.DataFrame(values,index=names,columns=[<span class="string">'coef'</span>]) <span class="comment"># 把所有项放入一个带标签的DataFrame中</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="逻辑回归（广义线性模型（GLM））"><a href="#逻辑回归（广义线性模型（GLM））" class="headerlink" title="逻辑回归（广义线性模型（GLM））"></a>逻辑回归（广义线性模型（GLM））</h2><p>当相应变量(y)不是连续的时候，更适合使用GLM模型，如逻辑回归、泊松回归等，特别的，当y是二值变量时，常用逻辑回归</p>
<h3 id="统计学库statsmodels-1"><a href="#统计学库statsmodels-1" class="headerlink" title="统计学库statsmodels"></a>统计学库statsmodels</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line">model = smf.logit(formula=<span class="string">'y ~ x1 + x2 +x3'</span>,data=df).fit() <span class="comment">#其中，y为二值变量</span></span><br><span class="line">model.sumamry() <span class="comment"># 结果汇总表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在逻辑回归中，与所有广义线性模型一样，都需要还有连接函数执行一定的转换，如需要解释逻辑模型，则需要将结果指数化</span></span><br><span class="line">odd_ratios = np.exp(model.params) <span class="comment"># 结果应该解释为如果x1每增加一倍，则y为1的概率就会增加多少倍（指数化后的系数值）</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="使用sklearn"><a href="#使用sklearn" class="headerlink" title="使用sklearn"></a>使用sklearn</h3><p>先手动创建虚拟变量get_dummies,再进行回归，然后把结果手动存储到标签，最后为了解释系数将值指数化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> linear_model <span class="keyword">from</span> sklearn</span><br><span class="line">predictors = pd.get_dummies(df[[<span class="string">'x1'</span>,<span class="string">'x2'</span>,<span class="string">'x3'</span>,<span class="string">'x4'</span>]],drop_first=<span class="literal">True</span>)</span><br><span class="line">lr = linear_model.LogisticRegrssion()</span><br><span class="line">results = lr.fit(X=predictors,y=df[<span class="string">'y'</span>])</span><br><span class="line">values = np.append(predicted.intercept_,predicted.coef_) <span class="comment"># 获取回归系数和截距项（常数项）</span></span><br><span class="line">names = np.append(<span class="string">'intercept'</span>,df_dummy_ref.columns)  <span class="comment"># 获取值对应名称</span></span><br><span class="line">results = pd.DataFrame(values,index=names,columns=[<span class="string">'coef'</span>]) <span class="comment"># 把所有项放入一个带标签的DataFrame中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了解释系数将值指数化</span></span><br><span class="line">results[<span class="string">'or'</span>] = np.exp(results[<span class="string">'coef'</span>])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="泊松回归（广义线性模型（GLM））"><a href="#泊松回归（广义线性模型（GLM））" class="headerlink" title="泊松回归（广义线性模型（GLM））"></a>泊松回归（广义线性模型（GLM））</h2><p>当响应变量y为计数数据时，常使用泊松回归，也可以使用更通用的glm函数来执行</p>
<h3 id="统计学库statsmodels-2"><a href="#统计学库statsmodels-2" class="headerlink" title="统计学库statsmodels"></a>统计学库statsmodels</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用poisson回归</span></span><br><span class="line">model = smf.poisson(formula=<span class="string">'y ~ x1 + x2 +x3'</span>,data=df).fit()</span><br><span class="line"><span class="comment"># 使用glm函数</span></span><br><span class="line">model = smf.glm(</span><br><span class="line">    formula=<span class="string">'y ~ x1 + x2 +x3'</span></span><br><span class="line">    ,data=df</span><br><span class="line">    ,family=sm.families.Poisson(sm.genmod.families.links.log)</span><br><span class="line">).fit()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="模型诊断"><a href="#模型诊断" class="headerlink" title="模型诊断"></a>模型诊断</h2><h3 id="比较模型本身好坏：残差"><a href="#比较模型本身好坏：残差" class="headerlink" title="比较模型本身好坏：残差"></a>比较模型本身好坏：残差</h3><p>残差是实际观测值和模型估计值之差<br>对于statsmodels，使用model.resid_deviance 读取回归模型的残差</p>
<h3 id="比较模型本身好坏：Q-Q图"><a href="#比较模型本身好坏：Q-Q图" class="headerlink" title="比较模型本身好坏：Q-Q图"></a>比较模型本身好坏：Q-Q图</h3><p>Q-Q图用于判断数据是否符合某个参考分布，许多模型都假设数据符合正态分布，因此可以用Q-Q图检验数据是否来自正态分布</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line">resid = model.resid_deviance.copy() <span class="comment"># 获取回归模型的残差</span></span><br><span class="line">resid_std = stats.zscore(resid)  <span class="comment"># 标准化</span></span><br><span class="line"></span><br><span class="line">fig = statsmodes.graphics.qqplot(resid,line=<span class="string">'r'</span>) <span class="comment"># 绘制QQ图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>还可以绘制残差直方图，用于判断是否符合正态分布</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fog,ax = plt.subplots()</span><br><span class="line">ax = sns.distplot(resid_std) <span class="comment"># 绘制残差直方图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="系数是否显著"><a href="#系数是否显著" class="headerlink" title="系数是否显著"></a>系数是否显著</h3><p>sklearn不支持查看系数的p-value，可以参考statsmodels模块。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line">X = diabetes.data</span><br><span class="line">y = diabetes.target</span><br><span class="line"></span><br><span class="line">X2 = sm.add_constant(X)</span><br><span class="line">est = sm.OLS(y, X2)</span><br><span class="line">est2 = est.fit()</span><br><span class="line">print(est2.summary())</span><br></pre></td></tr></tbody></table></figure>
<h3 id="比较模型之间好坏"><a href="#比较模型之间好坏" class="headerlink" title="比较模型之间好坏"></a>比较模型之间好坏</h3><h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><blockquote>
<p>方差分析会给出残差平方和RSS，残差平方和越小，模型拟合效果越好<br>AIC和BIC<br>（详见书第14章）</p>
</blockquote>
<h4 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h4><blockquote>
<p>方差分析<br>AIC和BIC</p>
</blockquote>
<h4 id="还可以使用K折交叉验证评判模型好坏"><a href="#还可以使用K折交叉验证评判模型好坏" class="headerlink" title="还可以使用K折交叉验证评判模型好坏"></a>还可以使用K折交叉验证评判模型好坏</h4><p>这种方法将数据分成K个部分，把其中一格部分用作测试集，其余部分用作训练集以拟合模型。<br>模型拟合好之后，使用测试集进行测试，并计算误差。不断重复这个过程，直到K个部分都测试过。<br>模型最终的误差是所有模型的平均值。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Tingting Hu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2020/11/11/学习笔记：Python线性回归学习笔记/">http://example.com/2020/11/11/学习笔记：Python线性回归学习笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/11/13/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B2%E8%A7%A3/"><i class="fa fa-chevron-left">  </i><span>数据分析与可视化讲解</span></a></div><div class="next-post pull-right"><a href="/2020/11/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9Agit%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span>git学习笔记</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/images/top.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By Tingting Hu</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>