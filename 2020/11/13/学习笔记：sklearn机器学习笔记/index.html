<!DOCTYPE html><html lang="zh_hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="sklearn机器学习笔记"><meta name="keywords" content="机器学习,sklearn"><meta name="author" content="Tingting Hu"><meta name="copyright" content="Tingting Hu"><title>sklearn机器学习笔记 | 南华路草堂</title><link rel="shortcut icon" href="/images/avatar.jpg"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.3.0'
} </script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="南华路草堂" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">1.重复值处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">1.2.</span> <span class="toc-text">2.缺失值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%97%B6%E9%97%B4%E5%8F%98%E9%87%8Fdatetime%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">3.时间变量datetime处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">4.处理分类变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%BF%9E%E7%BB%AD%E5%88%B0%E4%BA%8C%E5%88%86%E5%8F%98%E9%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">5.连续到二分变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.6.</span> <span class="toc-text">6.划分训练集和测试集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96%EF%BC%89"><span class="toc-number">1.7.</span> <span class="toc-text">7.数据标准化（无量纲化）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%98%AF%E5%90%A6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E6%98%AF"><span class="toc-number">1.7.1.</span> <span class="toc-text">训练集和测试集是否应该使用相同的预处理：是</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E6%97%B6%EF%BC%8C%E6%98%AF%E5%90%A6%E5%BA%94%E8%AF%A5%E5%B0%86%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%B7%B7%E5%90%88%E8%AE%A1%E7%AE%97%EF%BC%9A%E5%90%A6"><span class="toc-number">1.7.2.</span> <span class="toc-text">归一化时，是否应该将训练集和测试集混合计算：否</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#X%E5%92%8Cy%E5%88%86%E5%BC%80%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.7.3.</span> <span class="toc-text">X和y分开标准化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E9%80%898-%E5%AD%97%E6%AE%B5%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0-%E5%8F%AF%E5%BF%BD%E7%95%A5"><span class="toc-number">1.8.</span> <span class="toc-text">可选8.字段相关系数(可忽略)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SVF"><span class="toc-number">2.1.</span> <span class="toc-text">SVF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B"><span class="toc-number">2.2.</span> <span class="toc-text">线性回归预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">2.3.</span> <span class="toc-text">随机森林</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7"><span class="toc-number">3.</span> <span class="toc-text">模型评价</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%BB%E9%A2%84%E6%B5%8B%E5%80%BC%E5%92%8C%E7%9C%9F%E5%AE%9E%E5%80%BC%E5%9B%BE%E5%BD%A2"><span class="toc-number">3.1.</span> <span class="toc-text">画预测值和真实值图形</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="toc-number">3.2.</span> <span class="toc-text">学习曲线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">3.3.</span> <span class="toc-text">回归评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89"><span class="toc-number">3.3.1.</span> <span class="toc-text">均方误差（MSE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE%EF%BC%88RMSE%EF%BC%89"><span class="toc-number">3.3.2.</span> <span class="toc-text">均方根误差（RMSE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MAE-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.3.3.</span> <span class="toc-text">MAE(平均绝对误差)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#R-Squared"><span class="toc-number">3.3.4.</span> <span class="toc-text">R Squared</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/avatar.ico"></div><div class="author-info__name text-center">Tingting Hu</div><div class="author-info__description text-center">一天很长，十年很短</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">18</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">16</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/images/top.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">南华路草堂</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">sklearn机器学习笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-13</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><span>Reading time: 10 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>sklearn中规定必须导入数值型</p>
<h2 id="1-重复值处理"><a href="#1-重复值处理" class="headerlink" title="1.重复值处理"></a>1.重复值处理</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_df1 = df.drop_duplicates() <span class="comment"># 删除数据记录中所有列值相同的记录</span></span><br><span class="line">new_df2 = df.drop_duplicates([<span class="string">'col1'</span>]) <span class="comment"># 删除数据记录中col1值相同的记录</span></span><br><span class="line">new_df3 = df.drop_duplicates([<span class="string">'col2'</span>]) <span class="comment"># 删除数据记录中col2值相同的记录</span></span><br><span class="line">new_df4 = df.drop_duplicates([<span class="string">'col1'</span>, <span class="string">'col2'</span>]) <span class="comment"># 删除数据记录中指定列（col1/col2）值相同的记录</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-缺失值"><a href="#2-缺失值" class="headerlink" title="2.缺失值"></a>2.缺失值</h2><p>当样本量够大的时候，建议直接删除缺失数据行</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_new = df.dropna() <span class="comment">#不存在的值，删除整行</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="3-时间变量datetime处理"><a href="#3-时间变量datetime处理" class="headerlink" title="3.时间变量datetime处理"></a>3.时间变量datetime处理</h2><p>利用datetime自身性质拆分成年、月、日、时、分、秒</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">house_df[<span class="string">'deal_year'</span>] = house_df[<span class="string">'deal_dt'</span>].dt.year</span><br><span class="line">house_df[<span class="string">'deal_month'</span>] = house_df[<span class="string">'deal_dt'</span>].dt.month</span><br><span class="line">house_df[<span class="string">'deal_day'</span>] = house_df[<span class="string">'deal_dt'</span>].dt.day</span><br></pre></td></tr></tbody></table></figure>
<h2 id="4-处理分类变量"><a href="#4-处理分类变量" class="headerlink" title="4.处理分类变量"></a>4.处理分类变量</h2><p>采用pandas自带的get_dummies方法</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># drop_first会自动删除虚拟变量，避免多重共线性</span></span><br><span class="line">df_dummy_ref = pd.get_dummies(df[[<span class="string">'x1'</span>,<span class="string">'sex'</span>,<span class="string">'day'</span>]],drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="5-连续到二分变量"><a href="#5-连续到二分变量" class="headerlink" title="5.连续到二分变量"></a>5.连续到二分变量</h2><p>可采用pandas自带的cut方法</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'incoming'</span>] = pd.cut(df[<span class="string">'FamilyIncoming'</span>],[<span class="number">0</span>,<span class="number">100000</span>,df[<span class="string">'FamilyIncoming'</span>].<span class="built_in">max</span>()],labels=[<span class="number">0</span>,<span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="6-划分训练集和测试集"><a href="#6-划分训练集和测试集" class="headerlink" title="6.划分训练集和测试集"></a>6.划分训练集和测试集</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>,random_state=<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="7-数据标准化（无量纲化）"><a href="#7-数据标准化（无量纲化）" class="headerlink" title="7.数据标准化（无量纲化）"></a>7.数据标准化（无量纲化）</h2><p>譬如逻辑回归，支持向量机，神经网络，无量纲化可以加快求解速度；而在距离类模型，譬如K近邻，K-Means聚类中，无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。<br>（一个特例是决策树和树的集成算法们，对决策树我们不需要无量纲化，决策树可以把任意数据都处理得很好。）</p>
<p>数据的无量纲化可以是线性的，也可以是非线性的。线性的无量纲化包括中心化（Zero-centered或者Mean-subtraction）处理和缩放处理（Scale）。</p>
<blockquote>
<p>中心化的本质是让所有记录减去一个固定值，即让数据样本数据平移到某个位置。<br>缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理。</p>
</blockquote>
<p>数据归一化(Min-Max Scaling)。注意，Normalization是归一化，不是正则化。归一化之后的数据服从正态分布。<br>特殊的，当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)。</p>
<p>大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。</p>
<p>下面是对普通数组的标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line">print(data)</span><br><span class="line">scaler = StandardScaler()  <span class="comment"># 实例化</span></span><br><span class="line">scaler.fit(data)  <span class="comment"># fit，本质是生成均值和方差</span></span><br><span class="line">print(scaler.mean_)  <span class="comment"># 查看均值的属性mean_</span></span><br><span class="line">print(scaler.var_)  <span class="comment"># 查看方差的属性var_</span></span><br><span class="line"></span><br><span class="line">x_std = scaler.transform(data)  <span class="comment"># 通过接口导出结果</span></span><br><span class="line">print(x_std.mean())  <span class="comment"># 导出的结果是一个数组，用mean()查看均值</span></span><br><span class="line">print(x_std.std())  <span class="comment"># 用std()查看方差</span></span><br><span class="line"></span><br><span class="line">print(scaler.fit_transform(data))  <span class="comment"># 使用fit_transform(data)一步达成结果</span></span><br><span class="line">print(scaler.inverse_transform(x_std))  <span class="comment"># 使用inverse_transform逆转标准化</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练集和测试集是否应该使用相同的预处理：是"><a href="#训练集和测试集是否应该使用相同的预处理：是" class="headerlink" title="训练集和测试集是否应该使用相同的预处理：是"></a>训练集和测试集是否应该使用相同的预处理：是</h3><p>机器学习算法是建立在training和test data服从同一/类似分布的假设之上的。所以有一条准则：预处理对于training和test data要保持一致。如果预处理不同，那怎么还能保证training和test data分布一致</p>
<h3 id="归一化时，是否应该将训练集和测试集混合计算：否"><a href="#归一化时，是否应该将训练集和测试集混合计算：否" class="headerlink" title="归一化时，是否应该将训练集和测试集混合计算：否"></a>归一化时，是否应该将训练集和测试集混合计算：否</h3><p>在machine learning中，test data原则上是与training data独立的数据集。它的目的是验证在training data上训练好的模型是否在完全独立的，未知的数据集上有相同的表现。做归一化时，如果将test data和training data混合一起计算了，那么归一化之后的training data就相包含了test data的信息。这样就打破了test data应该与training data独立的原则。再者，在真正的工业应用中，你的test data可能是在你模型部署之后一个个采集的：有可能今天来10个，明天来6个，后天来1个。这种情况下如果将training和test data一起用来计算做归一化，那岂不是每天采集一次数据都要计算一次，再重新训练？</p>
<p>综合以上两点，在做归一化时，普遍的做法是只用training data计算数据如均值，方差，主成分，等，然后用保存好的计算数据对test data进行归一化。</p>
<h3 id="X和y分开标准化"><a href="#X和y分开标准化" class="headerlink" title="X和y分开标准化"></a>X和y分开标准化</h3><p>其实每一列就是分开的</p>
<p>下面是机器学习中的标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler_X = StandardScaler()</span><br><span class="line">X_train_z = scaler_X.fit_transform(X_train)</span><br><span class="line">scaler_y = StandardScaler()</span><br><span class="line">y_train_z = scaler_y.fit_transform(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练集的数据均值、方差归一化测试集，以便数据满足同分布</span></span><br><span class="line">X_test_z = scaler_X.fit_transform(X_test)</span><br><span class="line">y_test_z = scaler_y.fit_transform(y_test)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="可选8-字段相关系数-可忽略"><a href="#可选8-字段相关系数-可忽略" class="headerlink" title="可选8.字段相关系数(可忽略)"></a>可选8.字段相关系数(可忽略)</h2><p>当数据维度很多时可以考虑先查看相关系数，挑选相关性最强的维度进行训练，避免过拟合</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="SVF"><a href="#SVF" class="headerlink" title="SVF"></a>SVF</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从sklearn.svm中导入支持向量机回归模型SVR</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="comment">#1.使用线性核函数配置的支持向量机进行回归训练并预测</span></span><br><span class="line">linear_svr = SVR(kernel=<span class="string">'linear'</span>)</span><br><span class="line">linear_svr.fit(X_train,y_train)</span><br><span class="line">linear_svr_y_predict = linear_svr.predict(X_test)</span><br><span class="line"><span class="comment">#2.使用多项式核函数配置的支持向量机进行回归训练并预测</span></span><br><span class="line">poly_svr = SVR(kernel=<span class="string">'poly'</span>)</span><br><span class="line">poly_svr.fit(X_train,y_train)</span><br><span class="line">poly_svr_y_predict = poly_svr.predict(X_test)</span><br><span class="line"><span class="comment">#3.使用径向基核函数配置的支持向量机进行回归训练并预测</span></span><br><span class="line">rbf_svr = SVR(kernel=<span class="string">'rbf'</span>)</span><br><span class="line">rbf_svr.fit(X_train,y_train)</span><br><span class="line">rbf_svr_y_predict = rbf_svr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对三种核函数配置下的支持向量机回归模型在相同测试集下进行性能评估</span></span><br><span class="line"><span class="comment">#使用R-squared、MSE、MAE指标评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_absolute_error,mean_squared_error</span><br><span class="line"><span class="comment">#1.线性核函数配置的SVR</span></span><br><span class="line">print(<span class="string">'R-squared value of linear SVR is'</span>,linear_svr.score(X_test,y_test))</span><br><span class="line">print(<span class="string">'the MSE of linear SVR is'</span>,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(linear_svr_y_predict)))</span><br><span class="line">print(<span class="string">'the MAE of linear SVR is'</span>,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(linear_svr_y_predict)))</span><br><span class="line"><span class="comment">#2.多项式核函数配置的SVR</span></span><br><span class="line">print(<span class="string">'R-squared value of Poly SVR is'</span>,poly_svr.score(X_test,y_test))</span><br><span class="line">print(<span class="string">'the MSE of Poly SVR is'</span>,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(poly_svr_y_predict)))</span><br><span class="line"><span class="built_in">print</span> <span class="string">'the MAE of Poly SVR is'</span>,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(poly_svr_y_predict)))</span><br><span class="line"><span class="comment">#3.径向基核函数配置的SVR</span></span><br><span class="line">print(<span class="string">'R-squared value of RBF SVR is'</span>,rbf_svr.score(X_test,y_test)</span><br><span class="line">print(<span class="string">'the MSE of RBF SVR is'</span>,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(rbf_svr_y_predict)))</span><br><span class="line">print(<span class="string">'the MAE of RBF SVR is'</span>,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(rbf_svr_y_predict)))</span><br></pre></td></tr></tbody></table></figure>
<h2 id="线性回归预测"><a href="#线性回归预测" class="headerlink" title="线性回归预测"></a>线性回归预测</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选择基于梯度下降的线性回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">LR_reg = LinearRegression()</span><br><span class="line"><span class="comment">#进行拟合</span></span><br><span class="line">LR_reg.fit(X_train,y_train)</span><br><span class="line">LR_reg.predict(X_test)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line">rf = RandomForestRegressor()</span><br><span class="line">rf.fit(X_train,y_train)</span><br><span class="line">rf_t_pred = rf.predict(X_test)</span><br></pre></td></tr></tbody></table></figure>
<h1 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h1><h2 id="画预测值和真实值图形"><a href="#画预测值和真实值图形" class="headerlink" title="画预测值和真实值图形"></a>画预测值和真实值图形</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plot.figure(figsize=(<span class="number">10</span>,<span class="number">7</span>))       <span class="comment">#画布大小</span></span><br><span class="line">num=<span class="number">100</span></span><br><span class="line">x=np.arange(<span class="number">1</span>,num+<span class="number">1</span>)              <span class="comment">#取100个点进行比较</span></span><br><span class="line">plot.plot(x,target[:num],label=<span class="string">'target'</span>)      <span class="comment">#目标取值</span></span><br><span class="line">plot.plot(x,preds[:num],label=<span class="string">'preds'</span>)        <span class="comment">#预测取值</span></span><br><span class="line">plot.legend(loc=<span class="string">'upper right'</span>)  <span class="comment">#线条显示位置</span></span><br><span class="line">plot.show()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机选取100个真实值和预测值，进行绘图比较</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_y_pred_test</span>(<span class="params">y_tst,y_pred</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))  <span class="comment"># 画布大小</span></span><br><span class="line">    num = <span class="number">100</span></span><br><span class="line">    x = np.arange(<span class="number">1</span>, num + <span class="number">1</span>)  <span class="comment"># 取100个点进行比较</span></span><br><span class="line">    num_list = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(y_tst),size=<span class="number">100</span>)</span><br><span class="line">    y_tst_new = []</span><br><span class="line">    y_pred_new = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> num_list:</span><br><span class="line">        y_tst_new.append(y_tst.iloc[i])</span><br><span class="line">        y_pred_new.append(y_pred[i])</span><br><span class="line">    plt.plot(x,y_tst_new, label=<span class="string">'target'</span>)  <span class="comment"># 目标取值</span></span><br><span class="line">    plt.plot(x,y_pred_new, label=<span class="string">'predict'</span>)  <span class="comment"># 预测取值</span></span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)  <span class="comment"># 线条显示位置</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>在画学习曲线时，横轴为训练样本的数量，纵轴为准确率。学习曲线就是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。</p>
<p>当训练集和测试集的误差收敛但却很高时，为高偏差。<br>左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。我们可以增加模型参数，比如，构建更多的特征，减小正则项。 此时通过增加数据量是不起作用的。</p>
<p>当训练集和测试集的误差之间有大的差距时，为高方差。<br>当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。 我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。</p>
<p>理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。</p>
<p>首先定义画出学习曲线的方法，<br>核心就是调用了 sklearn.model_selection 的 learning_curve，<br>学习曲线返回的是 train_sizes, train_scores, test_scores，<br>画训练集的曲线时，横轴为 train_sizes, 纵轴为 train_scores_mean，<br>画测试集的曲线时，横轴为 train_sizes, 纵轴为 test_scores_mean</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line"><span class="comment">#cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为5份)</span></span><br><span class="line"><span class="comment"># model就是fit后的结果</span></span><br><span class="line"><span class="comment"># ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点</span></span><br><span class="line">train_size, train_score, test_score = learning_curve(model1, X_train, Y_train, cv=<span class="number">10</span>, train_sizes=np.linspace(<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">5</span>))  </span><br><span class="line"></span><br><span class="line">train_scores_mean = np.mean(train_score, axis=<span class="number">1</span>)</span><br><span class="line">train_scores_std = np.std(train_score, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_mean = np.mean(test_score, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_std = np.std(test_score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.fill_between(train_size, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"r"</span>)</span><br><span class="line">plt.fill_between(train_size, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</span><br><span class="line">plt.plot(train_size, train_scores_mean, <span class="string">'o--'</span>, color=<span class="string">"r"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">plt.plot(train_size, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line"></span><br><span class="line">plt.grid()</span><br><span class="line">plt.title(<span class="string">'Learn Curve'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="回归评价指标"><a href="#回归评价指标" class="headerlink" title="回归评价指标"></a>回归评价指标</h2><h3 id="均方误差（MSE）"><a href="#均方误差（MSE）" class="headerlink" title="均方误差（MSE）"></a>均方误差（MSE）</h3><p>MSE （Mean Squared Error）叫做均方误差，用（ 真实值-预测值 ）然后平方之后求和平均，也就是线性回归的损失函数，在线性回归的时候我们的目的就是让这个损失函数最小。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mean_squared_error(scaler_y.inverse_transform(y_test), scaler_y.inverse_transform(y_pred))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="均方根误差（RMSE）"><a href="#均方根误差（RMSE）" class="headerlink" title="均方根误差（RMSE）"></a>均方根误差（RMSE）</h3><p>即MSE开个根号么，其实实质是一样的，只不过用于数据更好的描述。<br>例如：要做房价预测，每平方是万元（真贵），我们预测结果也是万元。那么差值的平方单位应该是 千万级别的，因此最好就开个根号，我们误差的结果就跟我们数据是一个级别的了。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">np.sqrt(mean_squared_error(scaler_y.inverse_transform(y_test_rmse), scaler_y.inverse_transform(y_pred)))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MAE-平均绝对误差"><a href="#MAE-平均绝对误差" class="headerlink" title="MAE(平均绝对误差)"></a>MAE(平均绝对误差)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> median_absolute_error</span><br><span class="line">mean_absolute_error(scaler_y.inverse_transform(y_test), scaler_y.inverse_transform(y_pred))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="R-Squared"><a href="#R-Squared" class="headerlink" title="R Squared"></a>R Squared</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score(y_test,y_pred)</span><br></pre></td></tr></tbody></table></figure>
<p>1减去（均方误差MSE 除以 方差 ）</p>
<p>分子就是我们训练出的模型预测的所有误差。<br>分母就是不管什么我们猜的结果就是y的平均数。（瞎猜的误差）</p>
<p>如果结果是0，就说明我们的模型跟瞎猜差不多。<br>如果结果是1。就说明我们模型无错误。<br>如果结果是0-1之间的数，就是我们模型的好坏程度。<br>如果结果是负数。说明我们的模型还不如瞎猜。（其实导致这种情况说明我们的数据其实没有啥线性关系）</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Tingting Hu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2020/11/13/学习笔记：sklearn机器学习笔记/">http://example.com/2020/11/13/学习笔记：sklearn机器学习笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/sklearn/">sklearn</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/11/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ASelenium%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"><i class="fa fa-chevron-left">  </i><span>Selenium自动化测试</span></a></div><div class="next-post pull-right"><a href="/2020/11/13/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B2%E8%A7%A3/"><span>数据分析与可视化讲解课程学习</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/images/top.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By Tingting Hu</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>