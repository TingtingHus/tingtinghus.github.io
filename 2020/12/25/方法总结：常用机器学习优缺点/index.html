<!DOCTYPE html><html lang="zh_hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="常用机器学习优缺点"><meta name="keywords" content="机器学习"><meta name="author" content="Tingting Hu"><meta name="copyright" content="Tingting Hu"><title>常用机器学习优缺点 | 南华路草堂</title><link rel="shortcut icon" href="/images/avatar.jpg"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.3.0'
} </script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="南华路草堂" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-number">1.2.</span> <span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A-1"><span class="toc-number">2.1.</span> <span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A-1"><span class="toc-number">2.2.</span> <span class="toc-text">缺点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic%E5%9B%9E%E5%BD%92%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F%EF%BC%9A"><span class="toc-number">2.3.</span> <span class="toc-text">logistic回归应用领域：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KNN"><span class="toc-number">3.</span> <span class="toc-text">KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">3.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">3.2.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">3.3.</span> <span class="toc-text">KNN算法应用领域</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kmeans"><span class="toc-number">4.</span> <span class="toc-text">Kmeans</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-1"><span class="toc-number">4.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="toc-number">4.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">5.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-2"><span class="toc-number">5.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-2"><span class="toc-number">5.2.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E6%8E%AA%E6%96%BD"><span class="toc-number">5.3.</span> <span class="toc-text">改进措施</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">5.4.</span> <span class="toc-text">应用领域</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">6.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-3"><span class="toc-number">6.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-3"><span class="toc-number">6.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SVM"><span class="toc-number">7.</span> <span class="toc-text">SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-4"><span class="toc-number">7.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-4"><span class="toc-number">7.2.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">7.3.</span> <span class="toc-text">SVM应用领域</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">8.</span> <span class="toc-text">人工神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A-2"><span class="toc-number">8.1.</span> <span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A-2"><span class="toc-number">8.2.</span> <span class="toc-text">缺点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F%EF%BC%9A"><span class="toc-number">8.3.</span> <span class="toc-text">人工神经网络应用领域：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Adaboost"><span class="toc-number">9.</span> <span class="toc-text">Adaboost</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-5"><span class="toc-number">9.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-5"><span class="toc-number">9.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.</span> <span class="toc-text">其它模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E6%95%B0%E6%8D%AE%E5%AD%98%E5%9C%A8%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="toc-number">10.1.</span> <span class="toc-text">当数据存在多重共线性</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/avatar.ico"></div><div class="author-info__name text-center">Tingting Hu</div><div class="author-info__description text-center">一天很长，十年很短</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">20</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">17</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/images/top.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">南华路草堂</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">常用机器学习优缺点</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.2k</span><span class="post-meta__separator">|</span><span>Reading time: 12 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p><strong><em>本文是机器学习笔记的整理<br>参考来源有：<br>1.机器学习算法比较 <a target="_blank" rel="noopener" href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/">http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/</a><br>2.<a target="_blank" rel="noopener" href="https://blog.csdn.net/u010462995/article/details/70312702">https://blog.csdn.net/u010462995/article/details/70312702</a></em></strong></p>
<p>序号|算法|模型|监督学习|线性|多分类|求解算法|计算复杂度|解释性|缺失值影响|无量纲化的影响<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-|:-|:-<br>1|线性回归|判别|有|是|不支持|梯度下降法、正规方程解（解析解）|低|容易|敏感|加快速度<br>2|逻辑回归|判别|有|是|不支持|梯度下降法、牛顿法|低|容易|敏感|加快速度<br>3|朴素贝叶斯|生成|有|非|支持|公式解|中|容易|较不敏感|NA<br>4|KNN|判别|有|非|支持|Kd树算法|高|一般|一般|提升精度<br>5|Kmeans|判别|无|非|支持|误差平方和最小法|低|容易|一般|提升精度<br>6|决策树|判别|有|非|支持|贪心算法、穷举搜索|低|容易|不敏感|任何类型的数据<br>7|随机森林|判别|有|非|支持|贪心算法、穷举搜索|低|容易|不敏感<br>8|SVM|判别|有|非|不直接支持|SMO算法|中|容易|敏感|加快速度<br>9|人工神经网络|判别|有|非|支持|梯度下降法、反向传插法|高|困难|不敏感|加快速度<br>10|Adaboosting|判别|有|非|支持|分阶段优化、公式解|低|容易|不敏感|NA</p>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation（解方程）直接求得参数的解。</p>
<h2 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h2><p>实现简单，计算简单；</p>
<h2 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h2><p>不能拟合非线性数据</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关，与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，可以使用逻辑回归。</p>
<h2 id="优点：-1"><a href="#优点：-1" class="headerlink" title="优点："></a>优点：</h2><p>实现简单，广泛的应用于工业问题上；<br>分类时计算量非常小，速度很快，存储资源低；<br>便利的观测样本概率分数；<br>对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；<br>计算代价不高，易于理解和实现；</p>
<h2 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h2><p>当特征空间很大时，逻辑回归的性能不是很好；<br>容易欠拟合，一般准确度不太高<br>不能很好地处理大量多类特征或变量；<br>逻辑斯蒂回归对自变量的多元共线性非常敏感，要求自变量之间相互独立；<br>只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；<br>对于非线性特征，需要进行转换；</p>
<h2 id="logistic回归应用领域："><a href="#logistic回归应用领域：" class="headerlink" title="logistic回归应用领域："></a>logistic回归应用领域：</h2><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等<br>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等<br>信用评估，是否违约<br>是否为垃圾邮件<br>测量市场营销的成功度<br>预测某个产品的收益</p>
<h1 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h1><p>KNN即最近邻算法，其主要过程为：</p>
<ol>
<li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序(升序)；</li>
<li>选前k个最小距离的样本；</li>
<li>根据这k个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<p>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率.</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>理论成熟，思想简单，既可以用来做分类也可以用来做回归；<br>可用于非线性分类；<br>训练时间复杂度为O(n)；<br>对数据没有假设，准确度高，对outlier异常值不敏感；<br>KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练；<br>KNN理论简单，容易实现；</p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；<br>需要大量内存；<br>对于样本容量大的数据集计算量比较大（体现在距离计算上）；<br>样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多；<br>KNN每一次分类都会重新进行一次全局运算；<br>k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择；</p>
<h2 id="KNN算法应用领域"><a href="#KNN算法应用领域" class="headerlink" title="KNN算法应用领域"></a>KNN算法应用领域</h2><p>文本分类、模式识别、聚类分析，多分类领域</p>
<h1 id="Kmeans"><a href="#Kmeans" class="headerlink" title="Kmeans"></a>Kmeans</h1><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>不能自动识别类的个数(输入参数是k，即k个类),随机找k个点作为初始中心：c1, c2, … ck，初始聚类中心的选择对聚类结果的影响很大</p>
<h2 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h2><p>算法简单，容易实现 ；<br>算法速度很快；<br>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛。<br>算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。</p>
<h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h2><p>对数据类型要求较高，适合数值型数据；<br>可能收敛到局部最小值，在大规模数据上收敛较慢<br>分组的数目k是一个输入参数，不合适的k可能返回较差的结果。<br>对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；<br>不适合于发现非凸面形状的簇，或者大小差别很大的簇。<br>对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。</p>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。<br>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。</p>
<h2 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h2><p>决策树易于理解和解释，可以可视化分析，容易提取出规则；<br>可以同时处理标称型和数值型数据；<br>比较适合处理有缺失属性的样本；<br>能够处理不相关的特征；<br>测试数据集时，运行速度比较快；<br>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</p>
<h2 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h2><p>容易发生过拟合（随机森林可以很大程度上减少过拟合）；<br>容易忽略数据集中属性的相互关联；<br>对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。<br>ID3算法计算信息增益时结果偏向数值比较多的特征。</p>
<h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><p>对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。<br>使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题；</p>
<h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><p>企业管理实践，企业投资决策，由于决策树很好的分析能力，在决策过程应用较多。</p>
<h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><p>随机森林对多元共线性不敏感，结果对缺失数据和非平衡的数据比较稳健，可以很好地预测多达几千个解释变量的作用（Breiman 2001b），被誉为当前最好的算法之一。</p>
<h2 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h2><p>随机森林对<strong>多元共线性不敏感</strong>，结果对<strong>缺失数据</strong>和<strong>非平衡的数据</strong>比较稳健，可以很好地预测多达几千个解释变量的作用。<br>它的学习过程很快。在处理很大的数据时，它依旧非常高效<br>它包含估计缺失值的算法，如果有一部分的资料遗失，仍可以维持一定的准确度。<br>随机森林对<strong>离群值不敏感</strong>，在随机干扰较多的情况下表现稳健。<br>随机森林中分类树的算法自然地包括了变量的交互作用（interaction）（Cutler, etal.，2007），即X1的变化导致X2对Y的作用发生改变。交互作用在其他模型中（如逻辑斯蒂回归）因其复杂性经常被忽略。</p>
<h2 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h2><p>随机森林在解决回归问题时并没有像它在分类中表现的那么好<br>对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试<br>是它的算法倾向于观测值较多的类别</p>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。</p>
<h2 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h2><p>可以解决高维问题，即大型特征空间；<br>解决小样本下机器学习问题；<br>能够处理非线性特征的相互作用；<br>无局部极小值问题；（相对于神经网络等算法）<br>无需依赖整个数据；<br>泛化能力比较强；</p>
<h2 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h2><p>模型训练复杂度高；<br>当观测样本很多时，效率并不是很高；<br>对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；<br>对于核函数的高维映射解释力不强，尤其是径向基函数；<br>常规SVM只支持二分类，难以适应多分类问题；<br>对缺失数据敏感；<br>核函数选择没有较好的方法论，对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：<br>第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；<br>第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；<br>第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。<br>对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。</p>
<h2 id="SVM应用领域"><a href="#SVM应用领域" class="headerlink" title="SVM应用领域"></a>SVM应用领域</h2><p>文本分类、图像识别（主要二分类领域，毕竟常规SVM只能解决二分类问题）</p>
<h1 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h1><h2 id="优点：-2"><a href="#优点：-2" class="headerlink" title="优点："></a>优点：</h2><p>分类的准确度高；<br>并行分布处理能力强,分布存储及学习能力强，<br>对噪声神经有较强的鲁棒性和容错能力；<br>具备联想记忆的功能，能充分逼近复杂的非线性关系；</p>
<h2 id="缺点：-2"><a href="#缺点：-2" class="headerlink" title="缺点："></a>缺点：</h2><p>神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；<br>黑盒过程，不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；<br>学习时间过长，有可能陷入局部极小值，甚至可能达不到学习的目的。</p>
<h2 id="人工神经网络应用领域："><a href="#人工神经网络应用领域：" class="headerlink" title="人工神经网络应用领域："></a>人工神经网络应用领域：</h2><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p>
<h1 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h1><p>Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。有兴趣的同学可以阅读下自己之前写的这篇文章AdaBoost算法详述.下面总结下它的优缺点。</p>
<h2 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h2><p>Adaboost是一种有很高精度的分类器。<br>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br>当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。<br>简单，不用做特征筛选。<br>不易发生overfitting。<br>关于Adaboost, GBDT 及 XGBoost 算法区别，参考这篇文章：Adaboost、GBDT与XGBoost的区别</p>
<h2 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h2><p>对outlier比较敏感</p>
<h1 id="其它模型"><a href="#其它模型" class="headerlink" title="其它模型"></a>其它模型</h1><h2 id="当数据存在多重共线性"><a href="#当数据存在多重共线性" class="headerlink" title="当数据存在多重共线性"></a>当数据存在多重共线性</h2><p>在回归分析中，当自变量之间出现多重共线性现象时，常会严重影响到参数估计，扩大模型误差，并破坏模型的稳健性，因此消除多重共线性成为回归分析中参数估计的一个重要环节。现在常用的解决多元线性回归中多重共线性的回归模型有岭回归（Ridge Regression）、主成分回归(Principal Component Regression简记为PCR)和偏最小二乘回归(Partial Least Square Regression简记为PLS)。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Tingting Hu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2020/12/25/方法总结：常用机器学习优缺点/">http://example.com/2020/12/25/方法总结：常用机器学习优缺点/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/29/%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%EF%BC%9A%E6%80%8E%E6%A0%B7%E8%AF%84%E4%BB%B7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A5%BD%E5%9D%8F/"><i class="fa fa-chevron-left">  </i><span>怎样评价模型的好坏</span></a></div><div class="next-post pull-right"><a href="/2020/12/18/%E3%80%90%E5%AE%9E%E6%88%98%E3%80%91%22%E9%99%90%E4%BA%94%22%E6%96%B0%E6%94%BF%E5%AF%B9%E6%88%90%E9%83%BD%E4%BA%8C%E6%89%8B%E6%88%BF%E5%B8%82%E5%9C%BA%E5%BD%B1%E5%93%8D%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1/"><span>【项目实战】限五新政对成都二手房市场影响分析与机器学习建模</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/images/top.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By Tingting Hu</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>